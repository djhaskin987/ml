\documentclass[12pt]{article}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage[pdftex]{graphicx}
\pagestyle{plain}
\title{Back Propogation Project}
\author{Daniel Haskin}
\begin{document}
\maketitle
\section{Introduction}
Backpropogation is a method by which we may train an artificial nueral network
to classify data more and more accurately. In this project, I implement the
backpropogation algorithm, and use it to find out what choices of parameters
yield the best predictive accuracy using the Iris and Vowel datasets.
\section{Methods}
\subsection{Basic Implemented Functionality}
In my implementation of the backpropogation algorithm, I implemented
the following functionality:
\begin{enumerate}
\item The neural network updates its weights incrementally
\item Ability to add arbitrarily large layers to the network using the
    \texttt{--add-layer} option
\item Random weight initialization using a standard Gaussian distribution
\item Random shuffling (randomization) of the data at the beginning of each
    training epoch.
\item Optional momentum term via the \texttt{-m} option.
\end{enumerate}
\subsection{Stopping Criteria}
As to stopping criteria, I decided to look at the mean squared error (MSE) at
each epoch, or specifically its improvement. I kept track of the MSE on the
current epoch and also the MSE for the previous epoch is below threshold (chosen
as $0.001$), multiplied by the learning rate, I decided to stop. I included the
learning rate in the decision, reasoning that if the learning rate was small,
improvement over an epoch may be small enough to be under threshhold, yet very
far away from acheiving a minima with respect to MSE.
\section{Iris Problem}
\subsection{Choosing A Learning Rate}

As per the project specifications, I looked at several different learning rates
to determine the best learning rate for the iris dataset. My findings are
summarized in figures \ref{fig:learning_rate_accuracy} and
\ref{fig:learning_rate_epochs}.
\begin{figure}[!ht]
    \begin{center}
        \includegraphics[width=0.75\textwidth]{Iris-LearningRate-Accuracy}
    \end{center}
    \caption{Accuracy as a function of learning rate.}
  \label{fig:learning_rate_accuracy}
\end{figure}

\begin{figure}[!ht]
    \begin{center}
        \includegraphics[width=0.75\textwidth]{Iris-LearningRate-Epochs}
    \end{center}
    \caption{Number of epochs required to train as a function of learning rate.
    Note the log scale of the x-axis.}
  \label{fig:learning_rate_epochs}
\end{figure}

Looking at these graphs, it is clear that there is an accuracy-epoch trade-off.
When the learning rate is very small, the accuracy never seems to get worse,
though it seems there is diminishing return on accuracy as one decreases the
learning rate. However, for small learning rates, the number of epochs required
to train a dataset explodes as the learning rate increases, even when shown on
a log scale.

Looking at both graphs, it seems that $0.05$  seems like a good choice. It gets
very high accuracy, yet still has a manageable number of epochs before the
stopping criteria is satisfied.
\subsection

\end{document}
